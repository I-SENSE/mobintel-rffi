{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import json\n",
    "import xmltodict\n",
    "import h5py\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FRAMES = 500\n",
    "FRAMES_TRAIN = 500\n",
    "FRAMES_TEST = 100\n",
    "PKT_LEN = 400\n",
    "DIR_SOURCE = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/equalized_packets_min500frames/'\n",
    "FILE_TARGET_NON_EQ_TRAIN = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_non_eq_train.h5'\n",
    "FILE_TARGET_EQ_TRAIN = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_eq_train.h5'\n",
    "FILE_TARGET_NON_EQ_TEST = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_non_eq_test.h5'\n",
    "FILE_TARGET_EQ_TEST = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_eq_test.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-7 : 5212 NOTHING FOUND\n",
      "Nodes with Atheros 5212 WiFi card found: 47\n"
     ]
    }
   ],
   "source": [
    "ORBIT_DEVICE_INFO = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/orbit_device_info.json'\n",
    "\n",
    "def get_dataset_nodes(dir_source):\n",
    "    # Retrieves a list of node names which we have ready for H5 packaging\n",
    "    return [fname[8:-4] for fname in os.listdir(dir_source)]\n",
    "\n",
    "def get_orbit_node_capabilities(node_id, show = False):\n",
    "    url = f\"https://www.orbit-lab.org/cPanel/status/getNodeCapabilities?node=node{node_id}.grid.orbit-lab.org\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,uk-UA;q=0.8,uk;q=0.7,ru;q=0.6\",\n",
    "        \"Authorization\": \"Basic c21hem9raGE6LWkyMXB4OHR5cg==\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"trac_form_token=39202d14196f94e14ee8fca3; trac_auth=6865493b9d6768ff121dbaeba46347f5\",\n",
    "        \"Host\": \"www.orbit-lab.org\",\n",
    "        \"Referer\": \"https://www.orbit-lab.org/cPanel/status/template/index.html\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "        \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        responseJson = xmltodict.parse(response.text)\n",
    "        if show: print(json.dumps(responseJson, indent=4))\n",
    "        return responseJson\n",
    "    else: return None\n",
    "\n",
    "def save_dict_to_json_file(dictionary, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file, indent=4)\n",
    "\n",
    "def read_json_file_to_dict(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        dictionary = json.load(json_file)\n",
    "    return dictionary\n",
    "\n",
    "def contains_allowed_substring(input_string, allowed_substrings):\n",
    "    for substring in allowed_substrings:\n",
    "        if substring in input_string: return True\n",
    "    return False\n",
    "\n",
    "def get_orbit_node_infos(node_list, file_path):\n",
    "    node_infos = {}\n",
    "\n",
    "    for node_id in node_list:\n",
    "        print(\"Processing\", node_id)\n",
    "        node_info = get_orbit_node_capabilities(node_id)\n",
    "\n",
    "        if node_info is None:\n",
    "            print(node_id, ': nothing found')\n",
    "        else:\n",
    "            node_infos[node_id] = node_info['response']['action']['devices']['device']\n",
    "        \n",
    "    save_dict_to_json_file(node_infos, file_path)\n",
    "\n",
    "def filter_nodes_by_device_model(node_infos):\n",
    "    # Paper mentions that they were using Atheros 5212, 9220, 9280, and 9580 WiFi cards\n",
    "    # We need to find the largest number of nodes (for which we have sufficient data)\n",
    "    # with ONE of these cards on board (remember: we need the same hardware vendor for \n",
    "    # better model performance)\n",
    "    #\n",
    "    # After some experimentation, turns out that 5212 card is most common (47 devices w 500 frame limit)\n",
    "    #\n",
    "    # Additionally, card 5212 has one device. \n",
    "    # \n",
    "    # Also, uniqueness of the vendor/model can be identified using the @INV_dev_id field.\n",
    "\n",
    "    device_types_allowed = ['5212']\n",
    "\n",
    "    node_list_filtered = []\n",
    "    for node_id in node_infos:\n",
    "        node_info = node_infos[node_id]\n",
    "\n",
    "        node_fit_devices = 0\n",
    "        for device in node_info:\n",
    "            device_id = device.get('@INV_dev_id')\n",
    "            device_type = device.get(\"@INV_dev_type\")\n",
    "            device_name = device.get('@name')\n",
    "        \n",
    "            if contains_allowed_substring(device_type, device_types_allowed):\n",
    "                # print('[', device_id, ']:', node_id, ':', device_name, '(', device_type, ')')\n",
    "                node_fit_devices = node_fit_devices + 1\n",
    "\n",
    "        if node_fit_devices == 0:\n",
    "            print(node_id, ':', '5212 NOTHING FOUND')\n",
    "        elif node_fit_devices >= 1:\n",
    "            node_list_filtered.append(node_id)\n",
    "\n",
    "    print('Nodes with Atheros 5212 WiFi card found:', len(node_list_filtered))\n",
    "\n",
    "    return node_list_filtered\n",
    "\n",
    "node_list = get_dataset_nodes(DIR_SOURCE)\n",
    "# get_orbit_node_infos(node_list, file_path=ORBIT_DEVICE_INFO)\n",
    "node_infos = read_json_file_to_dict(file_path=ORBIT_DEVICE_INFO)\n",
    "node_list_filtered = filter_nodes_by_device_model(node_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 19-19\n",
      "Processing 14-7\n",
      "Processing 10-17\n",
      "Processing 16-1\n",
      "Processing 19-20\n",
      "Processing 10-7\n",
      "Processing 10-11\n",
      "Processing 1-19\n",
      "Processing 11-1\n",
      "Processing 13-3\n",
      "Processing 8-3\n",
      "Processing 16-16\n",
      "Processing 1-18\n",
      "Processing 20-15\n",
      "Processing 11-20\n",
      "Processing 14-10\n",
      "Processing 11-7\n",
      "Processing 6-15\n",
      "Processing 11-4\n",
      "Processing 4-10\n",
      "Processing 4-11\n",
      "Processing 3-18\n",
      "Processing 15-1\n",
      "Processing 20-12\n",
      "Processing 8-20\n",
      "Processing 1-10\n",
      "Processing 19-1\n",
      "Processing 11-17\n",
      "Processing 8-8\n",
      "Processing 1-12\n",
      "Processing 1-16\n",
      "Processing 4-1\n",
      "Processing 20-19\n",
      "Processing 3-13\n",
      "Processing 2-6\n",
      "Processing 6-1\n",
      "Processing 20-1\n",
      "Processing 8-18\n",
      "Processing 5-5\n",
      "Processing 7-11\n",
      "Processing 7-10\n",
      "Processing 7-14\n",
      "Processing 2-19\n",
      "Processing 5-1\n",
      "Processing 17-10\n",
      "Processing 12-20\n",
      "Processing 17-11\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_non_eq_train.h5\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_eq_train.h5\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_non_eq_test.h5\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_eq_test.h5\n"
     ]
    }
   ],
   "source": [
    "def save_dataset_h5(file_target, label, data):\n",
    "    print('Saving', file_target)\n",
    "    with h5py.File(file_target, 'w') as h5file:\n",
    "        h5file.create_dataset('label', data=label, dtype='float64')\n",
    "        h5file.create_dataset('data', data=data, dtype='float64')        \n",
    "\n",
    "def package_dataset_h5(node_names, dir_source, frame_count, sample_count):\n",
    "    L = len(node_names)\n",
    "\n",
    "    h5data_non_eq = np.zeros((L * frame_count, sample_count * 2), dtype='float64')\n",
    "    h5data_eq = np.zeros((L * frame_count, sample_count * 2), dtype='float64')\n",
    "    h5labels = np.zeros((L * frame_count, 1), dtype='float64')\n",
    "\n",
    "    h5_idx = 0\n",
    "    for node_idx in np.arange(len(node_names)):\n",
    "        node_name = node_names[node_idx]\n",
    "\n",
    "        print(\"Processing\", node_name)\n",
    "\n",
    "        f = scipy.io.loadmat(dir_source + 'packets_' + node_name, verify_compressed_data_integrity=False)\n",
    "        \n",
    "        # Retrieve the list of frames; each item is a cell, containing two vectors: non-eq & eq IQ samples\n",
    "        frames = f['packet_log'][0]\n",
    "\n",
    "        if len(frames) < frame_count:\n",
    "            print('Not enough frames for ', node_name)\n",
    "            continue\n",
    "\n",
    "        for frame_idx in np.arange(frame_count):\n",
    "            iq_non_eq = frames[frame_idx][0:frame_count, 0] # non-equalized\n",
    "            iq_eq = frames[frame_idx][0:frame_count, 1] # equalized\n",
    "\n",
    "            h5data_non_eq[h5_idx, 0::2] = np.real(iq_non_eq)\n",
    "            h5data_non_eq[h5_idx, 1::2] = np.imag(iq_non_eq)\n",
    "\n",
    "            h5data_eq[h5_idx, 0::2] = np.real(iq_eq)\n",
    "            h5data_eq[h5_idx, 1::2] = np.imag(iq_eq)\n",
    "\n",
    "            h5labels[h5_idx] = node_idx\n",
    "            \n",
    "            h5_idx = h5_idx + 1\n",
    "\n",
    "    return [h5data_non_eq, h5data_eq, h5labels]\n",
    "\n",
    "def process_save_rx(node_list_filtered, dir_source, frame_count, sample_count, file_target_non_eq_train, file_target_eq_train, file_target_non_eq_class, file_target_eq_class):\n",
    "    nodes_train = node_list_filtered[0:30] # use first 30 devices for training the model\n",
    "    nodes_class = node_list_filtered[30:] # use second 30 devices for testing the model\n",
    "\n",
    "    [h5data_non_eq_train, h5data_eq_train, h5labels_train] = package_dataset_h5(nodes_train, dir_source, frame_count, sample_count)\n",
    "    [h5data_non_eq_class, h5data_eq_class, h5labels_class] = package_dataset_h5(nodes_class, dir_source, frame_count, sample_count)\n",
    "\n",
    "    # Save to H5\n",
    "    save_dataset_h5(file_target=file_target_non_eq_train, label=h5labels_train, data=h5data_non_eq_train)\n",
    "    save_dataset_h5(file_target=file_target_eq_train, label=h5labels_train, data=h5data_eq_train)\n",
    "\n",
    "    save_dataset_h5(file_target=file_target_non_eq_class, label=h5labels_class, data=h5data_non_eq_class)\n",
    "    save_dataset_h5(file_target=file_target_eq_class, label=h5labels_class, data=h5data_eq_class)\n",
    "\n",
    "process_save_rx(node_list_filtered, DIR_SOURCE, MIN_FRAMES, PKT_LEN, FILE_TARGET_NON_EQ_TRAIN, FILE_TARGET_EQ_TRAIN, FILE_TARGET_NON_EQ_TEST, FILE_TARGET_EQ_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 800)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset_h5(file_target, dataset_name):\n",
    "    with h5py.File(file_target, 'r') as h5file:\n",
    "        dataset = h5file[dataset_name][:]\n",
    "    return dataset\n",
    "\n",
    "read_dataset_h5(FILE_TARGET_EQ_TEST, 'data').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
