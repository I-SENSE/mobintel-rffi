{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.io\n",
    "import os\n",
    "import json\n",
    "import xmltodict\n",
    "import h5py\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FRAMES = 500\n",
    "FRAMES_TRAIN = 500\n",
    "FRAMES_TEST = 100\n",
    "PKT_LEN = 400\n",
    "DIR_SOURCE = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/equalized_packets_min500frames/'\n",
    "# FILE_TARGET = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1.pkl'\n",
    "FILE_TARGET_NON_EQ = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_non_eq.h5'\n",
    "FILE_TARGET_EQ = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1/node1-1_eq.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_h5(file_target, label, data):\n",
    "    with h5py.File(file_target, 'w') as h5file:\n",
    "        h5file.create_dataset('label', label)\n",
    "        h5file.create_dataset('data', data)\n",
    "\n",
    "def process_save_rx(dir_source, file_target_non_eq, file_target_eq, frame_threshold, pkt_len):\n",
    "    # Contains a list of 3D np arrays for each of the transmitters\n",
    "    # Each array has a shape (L, N, 2)\n",
    "    # - L: number of frames captured (at least frame_threshold)\n",
    "    # - N: number of samples for each frame (size of the preamble at 25 Msps, 400)\n",
    "    # - 2: data is split into real & imag parts, hence the 2 instead of 1\n",
    "    data_non_eq = [] # not equalized\n",
    "    data_eq = [] # equalized\n",
    "    # Contains a list of names of each of the transmitters\n",
    "    node_list = [] \n",
    "\n",
    "    # Work trough each TX file with frames\n",
    "    for fname in os.listdir(dir_source):\n",
    "        f = scipy.io.loadmat(dir_source + fname, verify_compressed_data_integrity=False)\n",
    "        \n",
    "        # Retrieve the list of frames; each item is a cell, containing two vectors: non-eq & eq IQ samples\n",
    "        frames = f['packet_log'][0]\n",
    "        node_name = fname[8:-4]\n",
    "\n",
    "        # Don't process the file if not enough frames inside\n",
    "        if len(frames) < frame_threshold:\n",
    "            print(node_name,'Eliminated')\n",
    "            continue\n",
    "        # else: print(node_name, 'Processing')\n",
    "        \n",
    "        data_i_non_eq = np.zeros((frames.size, pkt_len, 2), dtype='float32')\n",
    "        data_i_eq = np.zeros((frames.size, pkt_len, 2), dtype='float32')\n",
    "\n",
    "        for frame_idx in np.arange(len(frames)):\n",
    "            # Separately save real & iamginary parts of IQ samples for both non-equalized & equalized versions\n",
    "            data_i_non_eq[frame_idx, :, 0] = np.real(frames[0][:, 0])\n",
    "            data_i_non_eq[frame_idx, :, 1] = np.imag(frames[0][:, 0])\n",
    "\n",
    "            data_i_eq[frame_idx, :, 0] = np.real(frames[0][:, 1])\n",
    "            data_i_eq[frame_idx, :, 1] = np.imag(frames[0][:, 1])\n",
    "                \n",
    "        data_non_eq.append(data_i_non_eq)\n",
    "        data_eq.append(data_i_eq)\n",
    "        node_list.append(node_name)\n",
    "\n",
    "    # If TX device is in the list of devices used for training the model\n",
    "\n",
    "    # If TX device is in the list of devices used for testing the model\n",
    "\n",
    "    # with open(file_target,'wb') as f:\n",
    "    #     pickle.dump({\n",
    "    #         'data_non_eq': data_non_eq,\n",
    "    #         'data_eq': data_eq,\n",
    "    #         'node_list': node_list\n",
    "    #     },f)\n",
    "\n",
    "    return [data_non_eq, data_eq, node_list]\n",
    "\n",
    "_, _, node_list = process_save_rx(DIR_SOURCE, FILE_TARGET_NON_EQ, FILE_TARGET_EQ, MIN_FRAMES, PKT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-7 : 5212 NOTHING FOUND\n",
      "Nodes with Atheros 5212 WiFi card found: 47\n"
     ]
    }
   ],
   "source": [
    "ORBIT_DEVICE_INFO = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/orbit_device_info.json'\n",
    "\n",
    "def get_orbit_node_capabilities(node_id, show = False):\n",
    "    url = f\"https://www.orbit-lab.org/cPanel/status/getNodeCapabilities?node=node{node_id}.grid.orbit-lab.org\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,uk-UA;q=0.8,uk;q=0.7,ru;q=0.6\",\n",
    "        \"Authorization\": \"Basic c21hem9raGE6LWkyMXB4OHR5cg==\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"trac_form_token=39202d14196f94e14ee8fca3; trac_auth=6865493b9d6768ff121dbaeba46347f5\",\n",
    "        \"Host\": \"www.orbit-lab.org\",\n",
    "        \"Referer\": \"https://www.orbit-lab.org/cPanel/status/template/index.html\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "        \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        responseJson = xmltodict.parse(response.text)\n",
    "        if show: print(json.dumps(responseJson, indent=4))\n",
    "        return responseJson\n",
    "    else: return None\n",
    "\n",
    "def save_dict_to_json_file(dictionary, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file, indent=4)\n",
    "\n",
    "def read_json_file_to_dict(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        dictionary = json.load(json_file)\n",
    "    return dictionary\n",
    "\n",
    "def contains_allowed_substring(input_string, allowed_substrings):\n",
    "    for substring in allowed_substrings:\n",
    "        if substring in input_string: return True\n",
    "    return False\n",
    "\n",
    "def get_orbit_node_infos(node_list, file_path):\n",
    "    node_infos = {}\n",
    "\n",
    "    for node_id in node_list:\n",
    "        print(\"Processing\", node_id)\n",
    "        node_info = get_orbit_node_capabilities(node_id)\n",
    "\n",
    "        if node_info is None:\n",
    "            print(node_id, ': nothing found')\n",
    "        else:\n",
    "            node_infos[node_id] = node_info['response']['action']['devices']['device']\n",
    "        \n",
    "    save_dict_to_json_file(node_infos, file_path)\n",
    "\n",
    "def filter_nodes_by_device_model(node_infos):\n",
    "    # Paper mentions that they were using Atheros 5212, 9220, 9280, and 9580 WiFi cards\n",
    "    # We need to find the largest number of nodes (for which we have sufficient data)\n",
    "    # with ONE of these cards on board (remember: we need the same hardware vendor for \n",
    "    # better model performance)\n",
    "    #\n",
    "    # After some experimentation, turns out that 5212 card is most common (47 devices w 500 frame limit)\n",
    "    #\n",
    "    # Additionally, card 5212 has one device. \n",
    "    # \n",
    "    # Also, uniqueness of the vendor/model can be identified using the @INV_dev_id field.\n",
    "\n",
    "    device_types_allowed = ['5212']\n",
    "\n",
    "    node_list_filtered = []\n",
    "    for node_id in node_infos:\n",
    "        node_info = node_infos[node_id]\n",
    "\n",
    "        node_fit_devices = 0\n",
    "        for device in node_info:\n",
    "            device_id = device.get('@INV_dev_id')\n",
    "            device_type = device.get(\"@INV_dev_type\")\n",
    "            device_name = device.get('@name')\n",
    "        \n",
    "            if contains_allowed_substring(device_type, device_types_allowed):\n",
    "                # print('[', device_id, ']:', node_id, ':', device_name, '(', device_type, ')')\n",
    "                node_fit_devices = node_fit_devices + 1\n",
    "\n",
    "        if node_fit_devices == 0:\n",
    "            print(node_id, ':', '5212 NOTHING FOUND')\n",
    "        elif node_fit_devices >= 1:\n",
    "            node_list_filtered.append(node_id)\n",
    "\n",
    "    print('Nodes with Atheros 5212 WiFi card found:', len(node_list_filtered))\n",
    "\n",
    "    return node_list_filtered\n",
    "\n",
    "# get_orbit_node_infos(node_list, file_path=ORBIT_DEVICE_INFO)\n",
    "node_infos = read_json_file_to_dict(file_path=ORBIT_DEVICE_INFO)\n",
    "node_list_filtered = filter_nodes_by_device_model(node_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2638385428.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[104], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def filter_nodes_by_device_model(node_infos)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
