{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import json\n",
    "import xmltodict\n",
    "import h5py\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FRAMES = 500\n",
    "FRAMES_TRAIN = 500\n",
    "FRAMES_TEST = 100\n",
    "PKT_LEN = 400\n",
    "\n",
    "ROOT_DIR = '/Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset'\n",
    "NODE_DIR = '/node1-1_wifi_2021_03_08'\n",
    "\n",
    "DIR_SOURCE = ROOT_DIR + NODE_DIR + '/equalized_packets_min500frames/'\n",
    "FILE_TARGET_NON_EQ_TRAIN = ROOT_DIR + NODE_DIR +'/node1-1_non_eq_train.h5'\n",
    "FILE_TARGET_EQ_TRAIN = ROOT_DIR + NODE_DIR + '/node1-1_eq_train.h5'\n",
    "FILE_TARGET_NON_EQ_TEST = ROOT_DIR + NODE_DIR + '/node1-1_non_eq_test.h5'\n",
    "FILE_TARGET_EQ_TEST = ROOT_DIR + NODE_DIR + '/node1-1_eq_test.h5'\n",
    "ORBIT_DEVICE_INFO = ROOT_DIR + '/orbit_device_info.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such node is not present in the directory.\n",
      "Such node is not present in the directory.\n",
      "Such node is not present in the directory.\n",
      "Such node is not present in the directory.\n",
      "Such node is not present in the directory.\n",
      "Such node is not present in the directory.\n",
      "20-7 : 5212 NOTHING FOUND\n",
      "Such node is not present in the directory.\n",
      "Nodes with Atheros 5212 WiFi card found: 40\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_nodes(dir_source):\n",
    "    # Retrieves a list of node names which we have ready for H5 packaging\n",
    "    return [fname[8:-4] for fname in os.listdir(dir_source)]\n",
    "\n",
    "def get_orbit_node_capabilities(node_id, show = False):\n",
    "    url = f\"https://www.orbit-lab.org/cPanel/status/getNodeCapabilities?node=node{node_id}.grid.orbit-lab.org\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9,uk-UA;q=0.8,uk;q=0.7,ru;q=0.6\",\n",
    "        \"Authorization\": \"Basic c21hem9raGE6LWkyMXB4OHR5cg==\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"trac_form_token=39202d14196f94e14ee8fca3; trac_auth=6865493b9d6768ff121dbaeba46347f5\",\n",
    "        \"Host\": \"www.orbit-lab.org\",\n",
    "        \"Referer\": \"https://www.orbit-lab.org/cPanel/status/template/index.html\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-origin\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "        \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        responseJson = xmltodict.parse(response.text)\n",
    "        if show: print(json.dumps(responseJson, indent=4))\n",
    "        return responseJson\n",
    "    else: return None\n",
    "\n",
    "def save_dict_to_json_file(dictionary, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(dictionary, json_file, indent=4)\n",
    "\n",
    "def read_json_file_to_dict(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        dictionary = json.load(json_file)\n",
    "    return dictionary\n",
    "\n",
    "def contains_allowed_substring(input_string, allowed_substrings):\n",
    "    for substring in allowed_substrings:\n",
    "        if substring in input_string: return True\n",
    "    return False\n",
    "\n",
    "def get_orbit_node_infos(node_list, file_path):\n",
    "    node_infos = {}\n",
    "\n",
    "    for node_id in node_list:\n",
    "        print(\"Processing\", node_id)\n",
    "        node_info = get_orbit_node_capabilities(node_id)\n",
    "\n",
    "        if node_info is None:\n",
    "            print(node_id, ': nothing found')\n",
    "        else:\n",
    "            node_infos[node_id] = node_info['response']['action']['devices']['device']\n",
    "        \n",
    "    save_dict_to_json_file(node_infos, file_path)\n",
    "\n",
    "def filter_nodes_by_device_model(dir_node_list, node_infos):\n",
    "    # Paper mentions that they were using Atheros 5212, 9220, 9280, and 9580 WiFi cards\n",
    "    # We need to find the largest number of nodes (for which we have sufficient data)\n",
    "    # with ONE of these cards on board (remember: we need the same hardware vendor for \n",
    "    # better model performance)\n",
    "    #\n",
    "    # After some experimentation, turns out that 5212 card is most common (47 devices w 500 frame limit)\n",
    "    #\n",
    "    # Additionally, card 5212 has one device. \n",
    "    # \n",
    "    # Also, uniqueness of the vendor/model can be identified using the @INV_dev_id field.\n",
    "\n",
    "    device_types_allowed = ['5212']\n",
    "\n",
    "    node_list_filtered = []\n",
    "    for node_id in node_infos:\n",
    "        if not dir_node_list.__contains__(node_id):\n",
    "            print('Such node is not present in the directory.')\n",
    "            continue\n",
    "\n",
    "        node_info = node_infos[node_id]\n",
    "\n",
    "        node_fit_devices = 0\n",
    "        for device in node_info:\n",
    "            device_id = device.get('@INV_dev_id')\n",
    "            device_type = device.get(\"@INV_dev_type\")\n",
    "            device_name = device.get('@name')\n",
    "        \n",
    "            if contains_allowed_substring(device_type, device_types_allowed):\n",
    "                # print('[', device_id, ']:', node_id, ':', device_name, '(', device_type, ')')\n",
    "                node_fit_devices = node_fit_devices + 1\n",
    "\n",
    "        if node_fit_devices == 0:\n",
    "            print(node_id, ':', '5212 NOTHING FOUND')\n",
    "        elif node_fit_devices >= 1:\n",
    "            node_list_filtered.append(node_id)\n",
    "\n",
    "    print('Nodes with Atheros 5212 WiFi card found:', len(node_list_filtered))\n",
    "\n",
    "    return node_list_filtered\n",
    "\n",
    "dir_node_list = get_dataset_nodes(DIR_SOURCE)\n",
    "# get_orbit_node_infos(dir_node_list, file_path=ORBIT_DEVICE_INFO)\n",
    "node_infos = read_json_file_to_dict(file_path=ORBIT_DEVICE_INFO)\n",
    "node_list_filtered = filter_nodes_by_device_model(dir_node_list, node_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_list_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 19-19\n",
      "Processing 14-7\n",
      "Processing 10-17\n",
      "Processing 16-1\n",
      "Processing 10-11\n",
      "Processing 13-3\n",
      "Processing 8-3\n",
      "Processing 16-16\n",
      "Processing 1-18\n",
      "Processing 20-15\n",
      "Processing 14-10\n",
      "Processing 11-7\n",
      "Processing 6-15\n",
      "Processing 11-4\n",
      "Processing 4-10\n",
      "Processing 3-18\n",
      "Processing 15-1\n",
      "Processing 20-12\n",
      "Processing 8-20\n",
      "Processing 1-10\n",
      "Processing 19-1\n",
      "Processing 11-17\n",
      "Processing 8-8\n",
      "Processing 1-12\n",
      "Processing 1-16\n",
      "Processing 4-1\n",
      "Processing 20-19\n",
      "Processing 3-13\n",
      "Processing 2-6\n",
      "Processing 20-1\n",
      "Processing 8-18\n",
      "Processing 5-5\n",
      "Processing 7-11\n",
      "Processing 7-10\n",
      "Processing 7-14\n",
      "Processing 2-19\n",
      "Processing 5-1\n",
      "Processing 17-10\n",
      "Processing 12-20\n",
      "Processing 17-11\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1_wifi_2021_03_08/node1-1_non_eq_train.h5\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1_wifi_2021_03_08/node1-1_eq_train.h5\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1_wifi_2021_03_08/node1-1_non_eq_test.h5\n",
      "Saving /Users/stepanmazokha/Desktop/wisig_frames_rffi_dataset/node1-1_wifi_2021_03_08/node1-1_eq_test.h5\n"
     ]
    }
   ],
   "source": [
    "def save_dataset_h5(file_target, label, data):\n",
    "    print('Saving', file_target)\n",
    "    with h5py.File(file_target, 'w') as h5file:\n",
    "        h5file.create_dataset('label', data=label, dtype='float64')\n",
    "        h5file.create_dataset('data', data=data, dtype='float64')        \n",
    "\n",
    "def package_dataset_h5(node_names, dir_source, frame_count, sample_count):\n",
    "    L = len(node_names)\n",
    "\n",
    "    h5data_non_eq = np.zeros((L * frame_count, sample_count * 2), dtype='float64')\n",
    "    h5data_eq = np.zeros((L * frame_count, sample_count * 2), dtype='float64')\n",
    "    h5labels = np.zeros((L * frame_count, 1), dtype='float64')\n",
    "\n",
    "    h5_idx = 0\n",
    "    for node_idx in np.arange(len(node_names)):\n",
    "        node_name = node_names[node_idx]\n",
    "\n",
    "        print(\"Processing\", node_name)\n",
    "\n",
    "        f = scipy.io.loadmat(dir_source + 'packets_' + node_name, verify_compressed_data_integrity=False)\n",
    "        \n",
    "        # Retrieve the list of frames; each item is a cell, containing two vectors: non-eq & eq IQ samples\n",
    "        frames = f['packet_log'][0]\n",
    "\n",
    "        if len(frames) < frame_count:\n",
    "            print('Not enough frames for ', node_name)\n",
    "            continue\n",
    "\n",
    "        for frame_idx in np.arange(frame_count):\n",
    "            iq_non_eq = frames[frame_idx][0:frame_count, 0] # non-equalized\n",
    "            iq_eq = frames[frame_idx][0:frame_count, 1] # equalized\n",
    "\n",
    "            h5data_non_eq[h5_idx, 0::2] = np.real(iq_non_eq)\n",
    "            h5data_non_eq[h5_idx, 1::2] = np.imag(iq_non_eq)\n",
    "\n",
    "            h5data_eq[h5_idx, 0::2] = np.real(iq_eq)\n",
    "            h5data_eq[h5_idx, 1::2] = np.imag(iq_eq)\n",
    "\n",
    "            h5labels[h5_idx] = node_idx\n",
    "            \n",
    "            h5_idx = h5_idx + 1\n",
    "\n",
    "    return [h5data_non_eq, h5data_eq, h5labels]\n",
    "\n",
    "def process_save_rx(node_list_filtered, dir_source, frame_count, sample_count, file_target_non_eq_train, file_target_eq_train, file_target_non_eq_class, file_target_eq_class):\n",
    "    nodes_train = node_list_filtered[0:30] # use first 30 devices for training the model\n",
    "    nodes_class = node_list_filtered[30:] # use second 30 devices for testing the model\n",
    "\n",
    "    [h5data_non_eq_train, h5data_eq_train, h5labels_train] = package_dataset_h5(nodes_train, dir_source, frame_count, sample_count)\n",
    "    [h5data_non_eq_class, h5data_eq_class, h5labels_class] = package_dataset_h5(nodes_class, dir_source, frame_count, sample_count)\n",
    "\n",
    "    # Save to H5\n",
    "    save_dataset_h5(file_target=file_target_non_eq_train, label=h5labels_train, data=h5data_non_eq_train)\n",
    "    save_dataset_h5(file_target=file_target_eq_train, label=h5labels_train, data=h5data_eq_train)\n",
    "\n",
    "    save_dataset_h5(file_target=file_target_non_eq_class, label=h5labels_class, data=h5data_non_eq_class)\n",
    "    save_dataset_h5(file_target=file_target_eq_class, label=h5labels_class, data=h5data_eq_class)\n",
    "\n",
    "process_save_rx(node_list_filtered, DIR_SOURCE, MIN_FRAMES, PKT_LEN, FILE_TARGET_NON_EQ_TRAIN, FILE_TARGET_EQ_TRAIN, FILE_TARGET_NON_EQ_TEST, FILE_TARGET_EQ_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 800)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset_h5(file_target, dataset_name):\n",
    "    with h5py.File(file_target, 'r') as h5file:\n",
    "        dataset = h5file[dataset_name][:]\n",
    "    return dataset\n",
    "\n",
    "read_dataset_h5(FILE_TARGET_EQ_TEST, 'data').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_size_calc(L, N):\n",
    "    # In STFT, with 50% overlap, we've got the following formula to determine size of the image:\n",
    "    # L = N + (N/2) * (M-1), where:\n",
    "    # - L: # of samples in the preamble (Shen had 8192)\n",
    "    # - N: nfft (Shen had it as 256)\n",
    "    # - M: # of windows\n",
    "    # Therefore, to estimate M we'll have the formula: \n",
    "    # M = 1 + (2/N) * (L - N)\n",
    "    return int(1 + (2/N) * (L - N))\n",
    "\n",
    "image_size_calc(8192, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.floor((8192-256)/128 + 1) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
