{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import h5py\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import boto3\n",
    "import matlab.engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATLAB_OFDM_DECODER = '/Users/stepanmazokha/Desktop/mobintel-rffi/preprocessor/frame_mac_detection'\n",
    "TEMP_IQ_DIRECTORY = '/Users/stepanmazokha/Desktop/orbit_processor_temp/'\n",
    "NODE_MACS = '/Users/stepanmazokha/Desktop/orbit_device_macs.json'\n",
    "\n",
    "S3_BUCKET_NAME = \"mobintel-orbit-dataset\"\n",
    "S3_EXPERIMENT_NAME = \"orbit_experiment_jul_19\"\n",
    "S3_EPOCH_PREFIX = \"epoch_\"\n",
    "S3_TRAINING_PREFIX = \"training_\"\n",
    "\n",
    "RFFI_DATASET_TARGET_DIR = f'/Users/stepanmazokha/Desktop/{S3_BUCKET_NAME}_h5/'\n",
    "\n",
    "FRAME_COUNT = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts signal configs from a file name in a dataset\n",
    "# - filename: name of the .dat file (without the route)\n",
    "def parse_dat_name(filename):\n",
    "    # Extract node_tx\n",
    "    node_tx_match = re.search(r'tx\\{node_(.*?)\\}', filename)\n",
    "    node_tx = node_tx_match.group(1) if node_tx_match else None\n",
    "\n",
    "    # Extract node_rx\n",
    "    node_rx_match = re.search(r'rx\\{node_(.*?)[\\+\\}]', filename)\n",
    "    node_rx = node_rx_match.group(1) if node_rx_match else None\n",
    "\n",
    "    # Extract samp_rate\n",
    "    samp_rate_match = re.search(r'rxSampRate_(\\d+e\\d+)', filename)\n",
    "    samp_rate = int(float(samp_rate_match.group(1))) if samp_rate_match else None\n",
    "\n",
    "    return {\n",
    "        \"node_tx\": node_tx,\n",
    "        \"node_rx\": node_rx,\n",
    "        \"samp_rate\": samp_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mateng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mateng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a JSON file containing MAC addresses of devices\n",
    "def get_device_macs(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressPercentage:\n",
    "    def __init__(self, filename, total_size):\n",
    "        self._filename = filename\n",
    "        self._total_size = total_size\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify, assume this is hooked up to a single filename\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._total_size) * 100\n",
    "            mb_downloaded = self._seen_so_far / (1024 * 1024)\n",
    "            mb_total = self._total_size / (1024 * 1024)\n",
    "            sys.stdout.write(\n",
    "                \"\\r%s  %.2f%% (%.2f MB of %.2f MB)\" % (self._filename, percentage, mb_downloaded, mb_total))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "def download_file_with_progress(bucket_name, s3_key, local_path):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Get the total size of the object\n",
    "    response = s3.head_object(Bucket=bucket_name, Key=s3_key)\n",
    "    total_size = response['ContentLength']\n",
    "\n",
    "    # Ensure the local directory exists\n",
    "    local_dir = os.path.dirname(local_path)\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    # Start the download and show the progress\n",
    "    s3.download_file(bucket_name, s3_key, local_path, Callback=ProgressPercentage(local_path, total_size))\n",
    "    \n",
    "def s3_list_subdirs(bucket_name, prefix):\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "    \n",
    "    subdirs = []\n",
    "    for path in response['CommonPrefixes']:\n",
    "        subdirs.append(os.path.basename(os.path.normpath(path['Prefix'])))\n",
    "    return subdirs\n",
    "\n",
    "def s3_list_files(bucket_name, prefix):\n",
    "    # Initialize the paginator\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "\n",
    "    # Create a PageIterator from the Paginator\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    # List to store all file keys\n",
    "    filenames = []\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page in page_iterator:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                filename = os.path.basename(os.path.normpath(obj['Key']))\n",
    "                if filename[-4:] == '.dat':\n",
    "                    filenames.append(filename)\n",
    "\n",
    "    return filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary which contains IDs (1--400) and (X-Y) of all sensors physically present\n",
    "# in the Orbit testbed facility. This is later used to produce unique labels for the sensor \n",
    "# fingerprinting model.\n",
    "def generate_node_ids():\n",
    "    ids = {}\n",
    "    node_i = 0\n",
    "    for i in np.arange(1, 21):\n",
    "        for j in np.arange(1, 21):\n",
    "            ids[str(i) + \"-\" + str(j)] = node_i\n",
    "            node_i = node_i + 1\n",
    "    return ids\n",
    "\n",
    "# Save an h5 dataset file containing labels & data for a given set of devices\n",
    "def save_dataset_h5(file_target, label, data):\n",
    "    print('Saving', file_target)\n",
    "    with h5py.File(file_target, 'w') as h5file:\n",
    "        h5file.create_dataset('label', data=label, dtype='float64')\n",
    "        h5file.create_dataset('data', data=data, dtype='float64')  \n",
    "\n",
    "# Package & store epoch infromation in h5 file (ready for RFFI)\n",
    "def epoch_save(node_ids_dict, target_dir, epoch_preambles):\n",
    "    for rx_name in epoch_preambles.keys():\n",
    "        rx_epochs = epoch_preambles[rx_name]\n",
    "\n",
    "        # Data shape: (epochs x frames, samples x 2)\n",
    "        # All frames/samples from all emitters are stitched together\n",
    "        h5_data = np.zeros((len(rx_epochs) * FRAME_COUNT, preamble_len * 2), dtype='float64')\n",
    "        # Labels shape: (epochs x frames, 1)\n",
    "        h5_labels = np.zeros((len(rx_epochs) * FRAME_COUNT, 1), dtype='float64')\n",
    "\n",
    "        h5_idx = 0\n",
    "        for rx_epoch in rx_epochs:\n",
    "            preambles = rx_epoch['preambles']\n",
    "            tx_node_name = rx_epoch['node_tx']\n",
    "            for preamble_i in np.arange(0, preambles.shape[0]):\n",
    "                h5_data[h5_idx, 0::2] = np.real(preambles[preamble_i, :])\n",
    "                h5_data[h5_idx, 1::2] = np.imag(preambles[preamble_i, :])\n",
    "\n",
    "                h5_labels[h5_idx] = node_ids_dict[tx_node_name]\n",
    "\n",
    "                h5_idx = h5_idx + 1\n",
    "\n",
    "\n",
    "        dataset_filepath = os.path.join(target_dir, f'node{rx_name}_{session_type}.h5')\n",
    "        save_dataset_h5(dataset_filepath, h5_labels, h5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping session silent\n",
      "Skipping session silent_2024-07-21_14-19-00\n",
      "Processing session  training_2024-07-20_00-50-38\n",
      "- tx{node_node1-10}_rx{node_node1-1+rxFreq_2462e6+rxGain_10+capLen_2+rxSampRate_25e6}.dat\n",
      "Downloading tx{node_node1-10}_rx{node_node1-1+rxFreq_2462e6+rxGain_10+capLen_2+rxSampRate_25e6}.dat...\n",
      "Resampling input waveform from 25 MHz to 20 MHz\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      ".............\n",
      "Searching for 00:60:b3:ac:a1:cb\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      "........................................\n",
      ".......................\n"
     ]
    }
   ],
   "source": [
    "# 1. Check if a directory to store final dataset exists and create if not\n",
    "if not os.path.exists(RFFI_DATASET_TARGET_DIR):\n",
    "    os.makedirs(RFFI_DATASET_TARGET_DIR)\n",
    "\n",
    "# 0. Set up the MATLAB environment before starting\n",
    "mateng = matlab.engine.start_matlab()\n",
    "mateng.cd(MATLAB_OFDM_DECODER, nargout=0)\n",
    "\n",
    "# 1. Load a JSON file with device MAC addresses\n",
    "device_macs = get_device_macs(NODE_MACS)\n",
    "\n",
    "# 2. Generate a dictionary of node IDs\n",
    "node_ids = generate_node_ids()\n",
    "\n",
    "# 1. Obtain a list of epochs in the experiment\n",
    "sessions = s3_list_subdirs(S3_BUCKET_NAME, S3_EXPERIMENT_NAME + '/')\n",
    "\n",
    "training_sessions = []\n",
    "testing_sessions = []\n",
    "for session_name in sessions:\n",
    "    if session_name[0:6] == 'epoch_':\n",
    "        testing_sessions.append(session_name)\n",
    "    elif session_name[0:9] == 'training_':\n",
    "        training_sessions.append(session_name)\n",
    "    else: print(\"Skipping session\", session_name)\n",
    "\n",
    "# 2. Process each training epoch\n",
    "# TODO: for session_name in training_sessions:\n",
    "session_name = \"training_2024-07-20_00-50-38\"\n",
    "session_type = \"train\"\n",
    "\n",
    "print(\"Processing session \", session_name)\n",
    "session_dat_files = s3_list_files(S3_BUCKET_NAME, S3_EXPERIMENT_NAME + \"/\" + session_name + \"/\")\n",
    "\n",
    "# 2.1. Prepare a dictionary to store preambles for this epoch\n",
    "epoch_preambles = defaultdict(list)\n",
    "rx_nodes = set()\n",
    "\n",
    "# 3. Process each .dat file\n",
    "for dat_file in session_dat_files:\n",
    "    print(f\"- {dat_file}\")\n",
    "\n",
    "    # 3.1. Download the file from S3\n",
    "    s3_filepath = f\"{S3_EXPERIMENT_NAME}/{session_name}/{dat_file}\"\n",
    "    local_filepath = os.path.join(TEMP_IQ_DIRECTORY, dat_file)\n",
    "    print(f'Downloading {dat_file}...')\n",
    "    # TODO: download_file_with_progress(S3_BUCKET_NAME, s3_filepath, local_filepath)\n",
    "\n",
    "    # 3.2. Extract signal info from its name\n",
    "    dat_config = parse_dat_name(dat_file)\n",
    "    tx_name = dat_config['node_tx'][4:]\n",
    "    rx_name = dat_config['node_rx'][4:]\n",
    "    samp_rate = dat_config['samp_rate']\n",
    "    preamble_len = 400 if samp_rate == 25e6 else 320 # samp rate can be 25e6 or 20e6\n",
    "\n",
    "    rx_nodes.add(rx_name)\n",
    "\n",
    "    # 3.3. Retrieve node MAC address\n",
    "    tx_mac = device_macs[tx_name]['mac']\n",
    "\n",
    "    # 3.2. Decode the file via MATLAB script, extract preambles\n",
    "    response = mateng.find_tx_frames(local_filepath, 'CBW20', samp_rate, tx_mac, preamble_len)\n",
    "    # preamble_bounds = np.array(response['preamble_bounds']).squeeze()\n",
    "    preamble_iq = np.array(response['preamble_iq']).squeeze()\n",
    "\n",
    "    if preamble_iq.shape[0] < FRAME_COUNT:\n",
    "        print(f\"Insufficient frames captured: {dat_file}\")\n",
    "        continue\n",
    "\n",
    "    # 3.3. Store information from a current dat file\n",
    "    epoch_preambles[rx_name].append({\n",
    "        'preambles': preamble_iq[0:FRAME_COUNT, :],\n",
    "        'node_tx': tx_name,\n",
    "        'node_rx': rx_name,\n",
    "        'node_mac': tx_mac,\n",
    "        'preamble_len': preamble_len\n",
    "    })\n",
    "\n",
    "    # 3.N. Remove local file afer the processing is completed\n",
    "    # TODO: print(f\"Deleting local file {local_filepath}\")\n",
    "    # TODO: os.remove(local_filepath)\n",
    "\n",
    "    break\n",
    "\n",
    "epoch_save(node_ids, RFFI_DATASET_TARGET_DIR, epoch_preambles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /Users/stepanmazokha/Desktop/mobintel-orbit-dataset_h5/node1-1_train.h5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 800)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset_h5(file_target, dataset_name):\n",
    "    with h5py.File(file_target, 'r') as h5file:\n",
    "        dataset = h5file[dataset_name][:]\n",
    "    return dataset\n",
    "\n",
    "a = read_dataset_h5('/Users/stepanmazokha/Desktop/mobintel-orbit-dataset_h5/node1-1_train.h5', 'data')\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
